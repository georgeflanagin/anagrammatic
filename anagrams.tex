\documentclass[letterpaper, 11pt]{article}
\usepackage{cmap}
\usepackage{urtechdoc}
\usepackage{urfooter}
\providecommand{\anagrammar}{A\textsc{nagrammar}\xspace}


\SetWatermarkText{}
\SetWatermarkFontSize{5cm}
\SetWatermarkAngle{58}
\SetWatermarkColor[gray]{0.99}

\title{An Exaustive Anagram Generator in Python\\
Using No String Comparisons\\ 
\Large \vspace{0.7em} (Only Integer Math and Trees)}
\author{George Flanagin\\University of Richmond\\\lit{gflanagin@richmond.edu}}
\date{\today}

\setlength{\parskip}{0.8em}
\setlength{\parindent}{0.8em}

\newcommand{\D}{$\mathfrak{D}$\xspace}
\newcommand{\Dp}{$\mathfrak{D}'$\xspace}
\newcommand{\Dpp}{$\mathfrak{D}''$\xspace}
\newcommand{\w}{$w$\xspace}
\newcommand{\aw}{$\overrightarrow{w}$\xspace}
\newcommand{\awp}{$\overrightarrow{w'}$\xspace}
\newcommand{\ra}{$\overrightarrow{r}$\xspace}
\newcommand{\rap}{$\overrightarrow{r'}$\xspace}

\sloppy

\begin{document}
\maketitle
\begin{abstract}
Anagrams are fun and difficult. They are an excellent way to explore
Python's data structures, and an excuse to develop a few new ones
expressly for this purpose. This paper defines an efficient algorithm
for finding all anagrams of a given collection of letters using the
Python Standard Library for the base code, and the system dictionary,
(or some other list of words)
as a source for allowed vocabulary. 

This program is the \anagrammar. Its source code and this documentation
may be found at \lit{github.com/georgeflanagin/anagrammatic}

\end{abstract}

\tableofcontents
\listoffigures

\newpage
\pagewiselinenumbers
\section{Getting started}
\quickquote{Of course just about every pronounceable combination
of five letters has been used to spell or misspell something somewhere, at
some point in history.}{Donald E. Knuth, 2011\\\emph{Combinatorial
Algorithms, Part 1}, p. 519\\answer to Problem 25 in Section 7}

\subsection{Definitions}
My interest in anagrams started later in life, at 37 years old. An episode of
\emph{The Simpsons} named ``Lisa's Rival'' aired 11 September 1994,
and in it Lisa visits a new girl in the school, Allison
Taylor. At Allison's house, Lisa is asked to join the Taylors in a
specialized game of anagrams in which the names of famous people
are rearranged to form descriptive anagrams: \emph{Alec Guinness}
becomes \emph{genuine class} in the show's example. Lisa was overwhelmed.

To avoid offending readers, I will be using my own
name as the basis for the examples in this paper. Mercifully for
the contruction of examples, my name consists only of common letters
in English, and it has a useful proportion of vowels. Let's call
the starting point the \emph{target phrase}.

To begin, we must have a definition of an anagram. Anagrams
always appear in pairs.  It makes no sense to say ``\emph{George
Flanagin} is an anagram," without stating its partner phrase, and the
one most suited to Lisa Simpson's game may be
\emph{long fearing age}. 

The technical properties of an anagram are:

\begin{enumerate}
\item Two phrases are anagrams of each other if and only if all the letters of
one phrase are present in the other, the same counts for each letter
present.
In the case of \emph{George Flanagin}, its
anagrams must have exactly three \emph{g}-s, two each of \emph{a},
\emph{e}, and \emph{n}, and one each of \emph{f}, \emph{i}, \emph{l},
\emph{o}, and \emph{r}.  Fourteen letters.

\item Each phrase in a pair of anagrams must consist of one or more
complete words in some dictionary.  Exactly which dictionary is a
point of {\ae}sthetics, as is whether one should consider single
letter words such as \lit{a} in English \emph{and} Spanish, \lit{I}
in English, and \lit{y}, \lit{o}, \lit{e}, \lit{u} in Spanish.  For
the purpose of this paper, we will ignore diacritics.

\item Additional rules are sometimes applied. Common constraints are:

\begin{itemize}
\item A maximum or minimum number of words in the anagrams of the original phrase,
and the related metric, minimum word length.

\item The elimination or inclusion of proper nouns in the derived
anagrams.

\item A lack of words shared with the original phrase. More discussion of this 
constraint lies ahead.
\end{itemize}
\end{enumerate}

Fortunately, none of the above boundaries is difficult to selectively
enforce in a program. 

\subsection{The big picture of the search}
\label{sec:bigpicture}

At first, the search seems terribly complicated, and it is certainly
not trivial. Everyone who has dealt with combinatorics knows how
quickly the the number of subsets grows, $16383$ in the case of
fourteen letters (\ie $2^{14}-1$), and the number of partitions, the fourteenth Bell
number, is $27,644,437$.\footnote{The Bell numbers are OEIS sequence
A000110, \lit{https://oeis.org/A000110}}

The constraint is the dictionary. The largest one generally used
in computing is the list of words that ships as the spelling
dictionary with Linux, and it has a mere $479826$ words. In any
search for anagrams except \emph{the quick brown fox jumped over
the lazy dog}, we will eliminate all the words that contain any
letter not found in the target. In the case of our example, we can
eliminate not only all the words that begin with $[b-d,h,j,k,m,n,p,q,s-z]$,
but all the words that contain one of those letters. Doing no
programming at all, we can put the upper limit on the number of
words to examine as $2777$ if only lower case words are considered,
and $5032$ if we allow words that normally have capitals in
English.\footnote{These numbers are the results of the two na\"ive
regular expression searches of the Linux spelling dictionary with
\lit{\textasciicircum[aegnfilor]\textbackslash+\$} in case sensitive
and case insensitive modes.} These are the upper bounds, because
the figures are not derived by looking for the words that have no
more of each letter than are found in the target.

\section{The algorithm}

\quickquote{Homer Jay, how do you keep your hair so rich and full?\\
Lather, rinse, and repeat. Always repeat.}{Homer Simpson\\D'oh-in'
in the Wind\\15 November 1998}

\subsection{Building a dictionary}

The algorithm in \anagrammar is all integer math, providing
simplicity and speed. 
Each letter in the alphabet is equated with a prime number, and it makes sense
to use the first 26 prime numbers, ${ 2, 3, .. 101}$. The order is 
unimportant, but choosing the order

\verb|eariotnslcudpmhgbfywkvxzjq|

tends to give the smallest numeric values for the products that represent
the words. The more familiar \verb|etaon...| order of frequency is based
on normal text in English, but we are concerned with getting our
text from a dictionary where each word occurs only once, and the above
sequence represents the frequency of letters of the words in a dictionary.
Raw experimentation suggests that the composite numbers representing
words have a reduced bit length of 15 to 20\%, which is substantial.

\begin{verbatim}
primes26 = (2, 3, 5, 7, 11,
    13, 17, 19, 23, 29,
    31, 37, 41, 43, 47,
    53, 59, 61, 67, 71,
    73, 79, 83, 89, 97,
    101 )
primes = dict(zip("eariotnslcudpmhgbfywkvxzjq", primes26))

def word_value(word:str) -> int:
    return math.prod(primes[_] for _ in word)
\end{verbatim}

Note that the fundamental theorem of arithmetic ensures that each
word is represented by a unique numeric value. However, many different
words may map to the \emph{same} value, for example, \lit{care}, \lit{acre},
and \lit{race} contain the same letters. If one of them is a part of
an anagram, then all of the others are also a part of that anagram.

A dictionary consists of a mapping, or in Python's language, a \lit{dict}, in
which the keys are the integers and the values are a set of words.

One additional convenience of this method is that we are only concerned
with the integer keys --- there is no need to examine the words until
the end of the process. This cuts down considerably on the memory
requirement for the running program.


\subsection{Select candidates from the dictionary}

Our first step is the elimination of all the words that are made
from incompatible collections of letters. In our grammar, we seek
to construct a collection of values that divide the value of the 
target phrase. This is more easily represented in Python than in 
conventional set notation from mathematics. 

In fact, it is so simple that there is no real benefit to making
it a function except for the clarity of notation. The \lit{filter}
argument is the phrase (or sub-phrase \ldots this is a recursive
algorithm) we are searching with the \lit{candidates}, a list
of potential factors.

\begin{verbatim}
def prune_dict(filter:int, candidates:tuple) -> tuple:
    return tuple(k for k in candidates if not filter % k)
\end{verbatim}

Given that this is a common operation in the \anagrammar, it is 
worth some future experimentation to see if it can be squeezed down
into fewer instructions.

\subsection{Trying the words}

It is useful to think of a target phrase and all its anagrams as an
n-ary tree,\footnote{n-ary trees are not a native citizen of Python,
and the discussion of the implementation of trees is discussed in 
Section~\ref{sec:trees}.} in which the root is the target phrase, and each path 
from the root to a leaf is an anagram. Using our chosen phrase,
\lit{georgeflanagin}, we can think of the relationship represented
like this:

\lit{root $\longrightarrow$ long $\longrightarrow$ fearing $\longrightarrow$ age}

\lit{~~~\textbackslash $\longrightarrow$ nine $\longrightarrow$ ear $\longrightarrow$ golf $\longrightarrow$ gag}

The obvious first step is to try all the words against the original
phrase and see what is left after the division. In our n-ary tree,
there will be at most 2777 branches from the root because there
are 2777 factors. Ugly? Yes, but we only need to build it rather than
plant it in the garden and admire it, and we will do our best to prune
as we go.

\subsection{Recursion}

Finding anagrams is a recursive process: if a word is a component of an anagram
for a given phrase,
then all of the word's anagrams are also elements of the phrase's set of
anagrams. For example, we can break apart \lit{fearing} into \lit{near-fig},
and substitute to have \lit{long-near-fig-age}.

Let's number the first connection to the root of the tree as edge zero, and
let's sort the factors that represent the words in ascending order. A key 
point is that the set of factors is not merely \emph{well ordered}, but 
has \emph{strict total ordering}: the factors are distinct, and they are all
comparable with $<$. 

In our programming, we can refer to the edge numbering as the \emph{depth}
because computer science trees are usually construed to grow from a root
in the sky toward the ground. Edges that branch from the initial test of
all the factors will be numbered $1$, and so on.

In our representation, there is some least factor that may or may
not correspond with the ``shortest word'' we are considering.  Our
representation has moved beyond the ambiguities of having several
three letter words and a reliance on a lexicographic ordering to
differentiate between them. We no longer need to test twice, once
for length and once for alpha order.

In Python, we can represent the sorted factors with this line of code,
where \lit{phrase\_v} is the value associated with the term we are
trying to decompose into an anagram of dictionary words.

\begin{verbatim}
factors = tuple(sorted(prune_dict(phrase_v, factors)))
\end{verbatim}
We can test the factors in a completely unoptimized for loop safely using 
integer division because the factors are all known to divide \lit{phrase\_v}
evenly --- that's what it means to be a factor.
\begin{verbatim}
for factor in factors:
    residual = phrase_v // factor
\end{verbatim}

The idea of recursion is simple enough to present as repeating the above
operation with the \lit{residual} and its factors that will be a proper
subset of the factors of \lit{phrase\_v}. Before we can recurse, we must
keep track of where we are in the tree.

\subsection{Bookkeeping}

\emph{Lather, rinse, repeat} is a risky process in programming because it requires
us to ensure we do not do the recursive operation infinitely, and there are many
things that will trip us up along the way. Let's use the name \lit{find\_words} for 
the recursive function. For each call to \lit{find\_words} we need to know the
level of recursion; this way we will add edges to the correct level of the
tree. This gives us a function signature of 

\begin{verbatim}
def find_words(phrase_v:int,
    factors:tuple,
    depth:int=0) -> SloppyTree:
\end{verbatim}

\section{Trees in Python}
\label{sec:trees}

When it comes to native data structures, Python has quite a few to choose
from, and all are useful. Along with \lit{list}, \lit{dict}, and \lit{tuple},
there are the many extended data structures in the \lit{collections} module. 
Nevertheless, any kind of tree is absent, and that may well be because there are so many 
different kinds of trees, enough that Donald E. Knuth has been giving an
annual lecture on new trees for the entire millennium, minus the Covid years of
2020 and 2021.

\anagrammar uses an n-ary tree implemented as a \lit{class} derived from \lit{dict}.
Unlike trees that require building nodes and linking them to other nodes, this tree
builds nodes automatically when they are first mentioned, and we have given
it the name \lit{Sloppy\-Tree}\texttrademark. \lit{Ugly\-Tree} might have been a 
better name, but the chosen name allows us to have a Python module with the
name \lit{slop.py}.

\lit{SloppyTree} is directly derived from \lit{dict}, and the
automatic node creation is achieved by having the \verb|__missing__|
function create a key whose value is an empty \lit{SloppyTree}.
Because subscript notation of deeply nested \lit{dict}s can become
tedious,

\begin{quote}
\begin{verbatim}
t['key1']['key2']['key3'] = 6
\end{verbatim}
\end{quote}

the \verb|__getattr__| and \verb|__setattr__| are added so that the
above line may be written as:

\begin{quote}
\begin{verbatim}
t.key1.key2.key3 = 6
\end{verbatim}
\end{quote}

In fact, there is no need to set a value to create the node. If you
are unsure of the value at the time the node is created, an
\emph{expression} like

\begin{quote}
\begin{verbatim}
t.key1.key2.key3
\end{verbatim}
\end{quote}

is really a \emph{statement} (!) that creates the cascade of needed
\lit{dict}s and their keys. It is equivalent to:

\begin{quote}
\begin{verbatim}
t = SloppyTree()
t['key1'] = SloppyTree()
t['key1']['key2'] = SloppyTree()
t['key1']['key2']['key3'] = SloppyTree()
\end{verbatim}
\end{quote}

\lit{SloppyTree} has a few additional features. An iterator for a
\lit{dict} only iterates over the keys, but we need to see the
values, and be able to distinguish between leaf and non-leaf nodes.
For its use in \anagrammar, we need an easy way to show all the anagrams,
which is to say, each path from the root to each leaf. Therefore,
we have an additional iterator/generator for these, through the
function \lit{SloppyTree.tree\_as\_table()}.



\vfill
\begin{figure}[b]
    \footnotesize
    \setlength\fboxsep{1cm}
    \setlength\fboxrule{0pt}
        {\raisebox{-1.2cm}{\includegraphics[width=0.14\textwidth]{urlogo.pdf}}}
    \hfill{}
    \begin{tabular}{ll}
        \multicolumn{2}{l}{\textbf{George Flanagin, Provost Office}}\\
        \multicolumn{2}{l}{\textbf{Academic Research Computing}}\\
        \hline \\ [-1.9ex]
        \textbf{Phone:}&+1.804.287.6392\\
        \textbf{Address:}&Richmond Hall, Office 104\\
        &114 UR Drive\\
        &\UR\\
        &Richmond, VA 23173\\
        \textbf{Email:}&gflanagin@richmond.edu\\ 
        \textbf{ORCID}&0000-0002-2084-5831\\
    \end{tabular}
\end{figure}

\end{document}


























